## Notes from "Deep Learning with PyTorch"

##### Author: Vishnu Subramanian

##### [Amazon](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-pytorch)

#### Types of Tensors
-  `torch.Tensor` is alias for default tensor type i.e `torch.FloatTensor`
-  `x = torch.Tensor([3.1]) # 0-D tensors aka scalars` - 1-D tensors with 1 element
-  `x = torch.FloatTensor([1,2,3,4,5])`
-  `b = np.array([[1,2,3]. [4,5,6], [7,8,9])` then `a = torch.from_numpy(b)` gives a 2-D tensor
-  `a = np.array(Image.open('img.jpg').resize(224,224))` then `a = torch.from_numpy(a)` gives 3-D tensor of dim `224*224*3`

- Tensors can be moved to GPU using .cuda() eg. `a = a.cuda()`
- Variable = Tensor (data) + gradient + reference to function that created it
- Gradient = rate of change of loss function wrt `(W,b)``])`

#### Grad Calculation
- `x = torch.tensor(torch.ones(2,2), requires_grad=True) `
- `y = x.mean()` # Makes y a 0-D Tensor with value 1 and grad_fn MeanBackward1
- `y.backward()` makes `x.grad` = [[1,1],[1,1]] # Before call to backward x.grad is None
- `x.grad.zero_()` # Zeroes the gradients

`d/dx` the variable on which backward is called, and that gives value of gradient of x.
- `z = x**2` # Makes z a 0-D Tensor with value 1 and grad_fn MeanBackward1
- `z.backward(gradient=torch.ones(z.size()))` makes `x.grad` = [[2,2],[2,2]]

- Also remember that gradients are accumulated. So if `y = x**2` and `z = x**3` and we call backward on both `z` and `y`, `x.grad` will be 5 (because 2+3). Hence zero-ing out gradients is necessary